{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_recognition.ipynb",
      "provenance": [],
      "mount_file_id": "1FzPJbqYDch8brc0_FTYAjyfys3jS6za4",
      "authorship_tag": "ABX9TyOWnSGyo49XoqI6QcMN41LZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgPlkh2s46i7"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Bidirectional, LSTM, Dense, Lambda, Activation, \\\n",
        "    BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKCxREQ75I27"
      },
      "source": [
        "# load dataset\n",
        "\n",
        "train_size = 30000\n",
        "valid_size = 3000\n",
        "\n",
        "train_x = np.load('/content/drive/MyDrive/text_recognition/train_x.npy')\n",
        "valid_x = np.load('/content/drive/MyDrive/text_recognition/valid_x.npy')\n",
        "\n",
        "\n",
        "train_y = np.load('/content/drive/MyDrive/text_recognition/train_y.npy')\n",
        "train_label_len = np.load('/content/drive/MyDrive/text_recognition/train_label_len.npy')\n",
        "train_input_len = np.load('/content/drive/MyDrive/text_recognition/train_input_len.npy')\n",
        "train_output = np.load('/content/drive/MyDrive/text_recognition/train_output.npy')\n",
        "\n",
        "\n",
        "valid_y = np.load('/content/drive/MyDrive/text_recognition/valid_y.npy')\n",
        "valid_label_len = np.load('/content/drive/MyDrive/text_recognition/valid_label_len.npy')\n",
        "valid_input_len = np.load('/content/drive/MyDrive/text_recognition/valid_input_len.npy')\n",
        "valid_output = np.load('/content/drive/MyDrive/text_recognition/valid_output.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sRNMyfM77Bj"
      },
      "source": [
        "def preprocess(img):\n",
        "    (h, w) = img.shape\n",
        "\n",
        "    final_img = np.ones([64, 256]) * 255  # blank white image\n",
        "\n",
        "    # crop\n",
        "    if w > 256:\n",
        "        img = img[:, :256]\n",
        "\n",
        "    if h > 64:\n",
        "        img = img[:64, :]\n",
        "\n",
        "    final_img[:h, :w] = img\n",
        "    return cv2.rotate(final_img, cv2.ROTATE_90_CLOCKWISE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB4t_amw989z"
      },
      "source": [
        "alphabets = u\"ABCDEFGHIJKLMNOPQRSTUVWXYZ-' \"\n",
        "max_str_len = 24  # max length of input labels\n",
        "num_of_characters = len(alphabets) + 1  # +1 for ctc pseudo blank\n",
        "num_of_timestamps = 64  # max length of predicted labels\n",
        "\n",
        "\n",
        "def label_to_num(label):\n",
        "    label_num = []\n",
        "    for ch in label:\n",
        "        label_num.append(alphabets.find(ch))\n",
        "\n",
        "    return np.array(label_num)\n",
        "\n",
        "\n",
        "def num_to_label(num):\n",
        "    ret = \"\"\n",
        "    for ch in num:\n",
        "        if ch == -1:  # CTC Blank\n",
        "            break\n",
        "        else:\n",
        "            ret += alphabets[ch]\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bijuUWF9-Cxo"
      },
      "source": [
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    # the 2 is critical here since the first couple outputs of the RNN\n",
        "    # tend to be garbage\n",
        "    y_pred = y_pred[:, 2:, :]\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQkXyPGV-54z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8498f321-9867-4279-bfba-058ed24b32f3"
      },
      "source": [
        "# model itself\n",
        "input_data = Input(shape=(256, 64, 1), name='input')\n",
        "\n",
        "inner = Conv2D(32, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(input_data)\n",
        "inner = BatchNormalization()(inner)\n",
        "inner = Activation('relu')(inner)\n",
        "inner = MaxPooling2D(pool_size=(2, 2), name='max1')(inner)\n",
        "\n",
        "inner = Conv2D(64, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)\n",
        "inner = BatchNormalization()(inner)\n",
        "inner = Activation('relu')(inner)\n",
        "inner = MaxPooling2D(pool_size=(2, 2), name='max2')(inner)\n",
        "inner = Dropout(0.3)(inner)\n",
        "\n",
        "inner = Conv2D(128, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(inner)\n",
        "inner = BatchNormalization()(inner)\n",
        "inner = Activation('relu')(inner)\n",
        "inner = MaxPooling2D(pool_size=(1, 2), name='max3')(inner)\n",
        "inner = Dropout(0.3)(inner)\n",
        "\n",
        "# CNN to RNN\n",
        "inner = Reshape(target_shape=((64, 1024)), name='reshape')(inner)\n",
        "inner = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)\n",
        "\n",
        "## RNN\n",
        "inner = Bidirectional(LSTM(256, return_sequences=True), name='lstm1')(inner)\n",
        "inner = Bidirectional(LSTM(256, return_sequences=True), name='lstm2')(inner)\n",
        "\n",
        "## OUTPUT\n",
        "inner = Dense(num_of_characters, kernel_initializer='he_normal', name='dense2')(inner)\n",
        "y_pred = Activation('softmax', name='softmax')(inner)\n",
        "\n",
        "model = Model(inputs=input_data, outputs=y_pred)\n",
        "\n",
        "labels = Input(name='gtruth_labels', shape=[max_str_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "ctc_loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
        "model_final = Model(inputs=[input_data, labels, input_length, label_length], outputs=ctc_loss)\n",
        "\n",
        "model_final.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=Adam(lr=0.0001))\n",
        "model_final.fit(x=[train_x, train_y, train_input_len, train_label_len], y=train_output,\n",
        "                validation_data=([valid_x, valid_y, valid_input_len, valid_label_len], valid_output), epochs=60, batch_size=128)\n",
        "\n",
        "model.save('/content/drive/MyDrive/text_recognition/right_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "235/235 [==============================] - 46s 195ms/step - loss: 24.6048 - val_loss: 20.6288\n",
            "Epoch 2/60\n",
            "235/235 [==============================] - 44s 189ms/step - loss: 20.1959 - val_loss: 20.0952\n",
            "Epoch 3/60\n",
            "235/235 [==============================] - 45s 189ms/step - loss: 19.8385 - val_loss: 19.6994\n",
            "Epoch 4/60\n",
            "235/235 [==============================] - 44s 189ms/step - loss: 19.3918 - val_loss: 19.2739\n",
            "Epoch 5/60\n",
            "235/235 [==============================] - 45s 190ms/step - loss: 18.6316 - val_loss: 18.8819\n",
            "Epoch 6/60\n",
            "235/235 [==============================] - 44s 188ms/step - loss: 17.7229 - val_loss: 17.8359\n",
            "Epoch 7/60\n",
            "235/235 [==============================] - 44s 189ms/step - loss: 16.5887 - val_loss: 16.1010\n",
            "Epoch 8/60\n",
            "235/235 [==============================] - 45s 190ms/step - loss: 14.8906 - val_loss: 14.1356\n",
            "Epoch 9/60\n",
            "235/235 [==============================] - 45s 190ms/step - loss: 12.8602 - val_loss: 11.5135\n",
            "Epoch 10/60\n",
            "235/235 [==============================] - 45s 190ms/step - loss: 10.8796 - val_loss: 9.6061\n",
            "Epoch 11/60\n",
            "235/235 [==============================] - 44s 189ms/step - loss: 9.2875 - val_loss: 8.2190\n",
            "Epoch 12/60\n",
            "235/235 [==============================] - 45s 190ms/step - loss: 7.9145 - val_loss: 6.7649\n",
            "Epoch 13/60\n",
            "235/235 [==============================] - 44s 189ms/step - loss: 6.7419 - val_loss: 5.8096\n",
            "Epoch 14/60\n",
            "235/235 [==============================] - 44s 188ms/step - loss: 5.8248 - val_loss: 5.1425\n",
            "Epoch 15/60\n",
            "235/235 [==============================] - 44s 188ms/step - loss: 5.2088 - val_loss: 4.4991\n",
            "Epoch 16/60\n",
            "235/235 [==============================] - 45s 190ms/step - loss: 4.7197 - val_loss: 4.2342\n",
            "Epoch 17/60\n",
            "235/235 [==============================] - 44s 189ms/step - loss: 4.3409 - val_loss: 4.1826\n",
            "Epoch 18/60\n",
            "235/235 [==============================] - 44s 188ms/step - loss: 4.0308 - val_loss: 3.5717\n",
            "Epoch 19/60\n",
            "235/235 [==============================] - 44s 188ms/step - loss: 3.7802 - val_loss: 3.3881\n",
            "Epoch 20/60\n",
            "235/235 [==============================] - 44s 188ms/step - loss: 3.5733 - val_loss: 3.2360\n",
            "Epoch 21/60\n",
            "235/235 [==============================] - 44s 187ms/step - loss: 3.3830 - val_loss: 3.0700\n",
            "Epoch 22/60\n",
            "235/235 [==============================] - 44s 187ms/step - loss: 3.2095 - val_loss: 3.0070\n",
            "Epoch 23/60\n",
            "235/235 [==============================] - 44s 187ms/step - loss: 3.0699 - val_loss: 2.8560\n",
            "Epoch 24/60\n",
            "235/235 [==============================] - 44s 187ms/step - loss: 2.9305 - val_loss: 2.8209\n",
            "Epoch 25/60\n",
            "235/235 [==============================] - 44s 187ms/step - loss: 2.8322 - val_loss: 2.6905\n",
            "Epoch 26/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 2.7356 - val_loss: 2.6591\n",
            "Epoch 27/60\n",
            "235/235 [==============================] - 44s 186ms/step - loss: 2.6377 - val_loss: 2.6061\n",
            "Epoch 28/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 2.5568 - val_loss: 2.5942\n",
            "Epoch 29/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 2.4762 - val_loss: 2.5115\n",
            "Epoch 30/60\n",
            "235/235 [==============================] - 44s 186ms/step - loss: 2.4032 - val_loss: 2.4280\n",
            "Epoch 31/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 2.3425 - val_loss: 2.3587\n",
            "Epoch 32/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 2.2687 - val_loss: 2.3415\n",
            "Epoch 33/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 2.2184 - val_loss: 2.3210\n",
            "Epoch 34/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 2.1530 - val_loss: 2.2695\n",
            "Epoch 35/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 2.1058 - val_loss: 2.2708\n",
            "Epoch 36/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 2.0672 - val_loss: 2.1941\n",
            "Epoch 37/60\n",
            "235/235 [==============================] - 44s 186ms/step - loss: 2.0102 - val_loss: 2.1939\n",
            "Epoch 38/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 1.9587 - val_loss: 2.1833\n",
            "Epoch 39/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 1.9189 - val_loss: 2.1453\n",
            "Epoch 40/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 1.8882 - val_loss: 2.1159\n",
            "Epoch 41/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 1.8334 - val_loss: 2.1214\n",
            "Epoch 42/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 1.7985 - val_loss: 2.0887\n",
            "Epoch 43/60\n",
            "235/235 [==============================] - 44s 186ms/step - loss: 1.7724 - val_loss: 2.0805\n",
            "Epoch 44/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 1.7222 - val_loss: 2.0688\n",
            "Epoch 45/60\n",
            "235/235 [==============================] - 44s 186ms/step - loss: 1.7002 - val_loss: 2.0331\n",
            "Epoch 46/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 1.6578 - val_loss: 2.0971\n",
            "Epoch 47/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 1.6242 - val_loss: 2.0386\n",
            "Epoch 48/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 1.6004 - val_loss: 2.0571\n",
            "Epoch 49/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 1.5787 - val_loss: 2.0075\n",
            "Epoch 50/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 1.5441 - val_loss: 2.0151\n",
            "Epoch 51/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 1.5151 - val_loss: 1.9879\n",
            "Epoch 52/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 1.4701 - val_loss: 2.1300\n",
            "Epoch 53/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 1.4446 - val_loss: 2.0590\n",
            "Epoch 54/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 1.4126 - val_loss: 2.0417\n",
            "Epoch 55/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 1.3923 - val_loss: 2.0362\n",
            "Epoch 56/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 1.3737 - val_loss: 1.9930\n",
            "Epoch 57/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 1.3403 - val_loss: 2.0057\n",
            "Epoch 58/60\n",
            "235/235 [==============================] - 43s 185ms/step - loss: 1.3097 - val_loss: 2.0268\n",
            "Epoch 59/60\n",
            "235/235 [==============================] - 44s 185ms/step - loss: 1.2818 - val_loss: 1.9939\n",
            "Epoch 60/60\n",
            "235/235 [==============================] - 43s 184ms/step - loss: 1.2644 - val_loss: 2.0603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raYm8vSj_IKI"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}